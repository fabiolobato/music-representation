{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-07-02T20:50:56.715790Z",
     "iopub.status.busy": "2021-07-02T20:50:56.713928Z",
     "iopub.status.idle": "2021-07-02T20:51:08.804893Z",
     "shell.execute_reply": "2021-07-02T20:51:08.804271Z"
    },
    "papermill": {
     "duration": 12.102583,
     "end_time": "2021-07-02T20:51:08.805063",
     "exception": false,
     "start_time": "2021-07-02T20:50:56.702480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GRU\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import Embedding, MaxPooling1D, GlobalAveragePooling1D, GlobalMaxPool1D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#metricas de avaliacao\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) \n",
    "    \n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
    "    return text\n",
    "\n",
    "df = pd.read_csv('../input/music-mar21/rock_mar21.csv')\n",
    "\n",
    "def split_feature_label(data):\n",
    "    X = data['letras'].apply(clean_text)\n",
    "    y = data['genero']\n",
    "    return X,y\n",
    "\n",
    "def two_places(value):\n",
    "    rounded= float(\"{:.2f}\".format(value))\n",
    "    return rounded\n",
    "\n",
    "#Define o nome do arquivo de saída (compilação de todos os resultados)\n",
    "def write(row):\n",
    "    row = [str(w) for w in row]\n",
    "    output = open(\"./results_rock_gru_return_sequences_max_pooling_glove.txt\",\"a\")#append mode\n",
    "    output.write(','.join(row)+\"\\n\")\n",
    "    output.close()\n",
    "\n",
    "def write_header(target_names):\n",
    "    header = ['nn','nn_params','word_embeddings_params','accuracy','f1_macro','f1_weighted']\n",
    "    for target in target_names:\n",
    "        header.append('{}_precision'.format(target))\n",
    "        header.append('{}_recall'.format(target))\n",
    "        header.append('{}_f1_score'.format(target))\n",
    "        header.append('{}_samples'.format(target))\n",
    "\n",
    "    header.append('total_samples')\n",
    "    write(header)\n",
    "\n",
    "\n",
    "def compile_results(nn,nn_params,custom_we,y_test, y_pred, target_names):\n",
    "    report = classification_report(y_test, y_pred, target_names=target_names, output_dict=True)\n",
    "    results_row = []\n",
    "    \n",
    "    results_row.append(nn)\n",
    "    results_row.append(nn_params)\n",
    "    results_row.append(custom_we)\n",
    "    results_row.append(two_places(report['accuracy']))\n",
    "    results_row.append(two_places(report['macro avg']['f1-score']))\n",
    "    results_row.append(two_places(report['weighted avg']['f1-score']))\n",
    "    for target in target_names:\n",
    "        results_row.append(two_places(report[target]['precision']))\n",
    "        results_row.append(two_places(report[target]['recall']))\n",
    "        results_row.append(two_places(report[target]['f1-score']))\n",
    "        results_row.append(report[target]['support'])\n",
    "\n",
    "    results_row.append(report['macro avg']['support'])\n",
    "    \n",
    "    write(results_row)\n",
    "\n",
    "X,y = split_feature_label(df)\n",
    "\n",
    "#Codificando as classes para um vetor numérico \n",
    "encoder = LabelEncoder()\n",
    "y_encoded = encoder.fit_transform(y)\n",
    "\n",
    "write_header(encoder.classes_)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded)\n",
    "\n",
    "num_classes = len(y.unique())\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(X)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "max_length = 200\n",
    "\n",
    "X_train_tokenized = t.texts_to_sequences(X_train)\n",
    "X_test_tokenized = t.texts_to_sequences(X_test)\n",
    "\n",
    "#Preenchimento da sequencia de textos para todos ficarem do mesmo tamanho\n",
    "#Isso é necessário para deixar todas as sequencias com o mesmo tamanho para que possam ser aplicadas à rede neural\n",
    "X_train_final = keras.preprocessing.sequence.pad_sequences(X_train_tokenized,maxlen=max_length, padding='post')\n",
    "X_test_final = keras.preprocessing.sequence.pad_sequences(X_test_tokenized,maxlen=max_length, padding='post')\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "#Aqui existem duas linhas custom_word_embeddings, a primeira carrega o diretório das embeddings pré-treinadas\n",
    "#no corpus\n",
    "#a segunda, eu carrego a embedding pré-treinada Glove\n",
    "\n",
    "#custom_word_embeddings = []\n",
    "\n",
    "#models = ['sg','cbow','fasttext'] \n",
    "#window_sizes = [5, 10]\n",
    "#num_dimensions = [100, 300]\n",
    "#num_max_epochs = [5, 50]\n",
    "\n",
    "#for model in models:\n",
    "    #for window_size in window_sizes:\n",
    "        #for dim_size in num_dimensions:\n",
    "            #for max_epochs in num_max_epochs:\n",
    "                 #custom_word_embeddings.append('../input/embedding-matrix/word embedding weight matrix/rock/custom_rock_{}_{}_{}_{}d.txt'.format(model,max_epochs,window_size,dim_size))\n",
    "\n",
    "\n",
    "custom_word_embeddings = glob('../input/glove-6b/*.txt')\n",
    "\n",
    "def cnn():\n",
    "    #REDE NEURAL CNN\n",
    "    filters_list = [50,100]\n",
    "    kernel_size_list = [2,3,4,5]\n",
    "\n",
    "    for word_embeddings in custom_word_embeddings:\n",
    "        for conv1d_filter in filters_list:\n",
    "            for conv1d_kernel in kernel_size_list:\n",
    "                \n",
    "                # load the whole embedding into memory\n",
    "                embeddings_index = dict()\n",
    "                f = open(word_embeddings)\n",
    "                for line in f:\n",
    "                    values = line.split()\n",
    "                    word = values[0]\n",
    "                    coefs = np.asarray(values[1:], dtype='float32')\n",
    "                    embeddings_index[word] = coefs\n",
    "                f.close()\n",
    "\n",
    "                #Número de dimensões da W.E.\n",
    "                num_dim = len(coefs)\n",
    "\n",
    "                #print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "                # create a weight matrix for words in training docs\n",
    "                embedding_matrix = np.zeros((vocab_size, num_dim))\n",
    "                for word, i in t.word_index.items():\n",
    "                    embedding_vector = embeddings_index.get(word)\n",
    "                    if embedding_vector is not None:\n",
    "                        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "                custom_embedding_name = word_embeddings.split('/')[-1].replace('.txt','')\n",
    "\n",
    "                nn_params = \"Embedding_out_dim = {}| Conv1D_filters={}| Conv1D_kernel_size={}\".format(num_dim, conv1d_filter, conv1d_kernel)\n",
    "                \n",
    "                model = Sequential()\n",
    "                model.add(Embedding(vocab_size, num_dim, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "                model.add(Conv1D(conv1d_filter, conv1d_kernel, activation='relu'))\n",
    "                model.add(MaxPooling1D(3))\n",
    "                model.add(Flatten())\n",
    "                model.add(Dense(num_classes, activation='softmax', kernel_regularizer=keras.regularizers.L2(0.001)))\n",
    "                #model.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "                model.compile(loss='mse', optimizer='adam')\n",
    "                #model.summary()\n",
    "                #print()\n",
    "                history = model.fit(X_train_final, y_train, epochs=100, batch_size=32, validation_data=(X_test_final,y_test), verbose=0, shuffle=False)\n",
    "                #avaliando a rede\n",
    "                #loss, accuracy = model.evaluate(X_test_final, y_test)\n",
    "                #print(\"Acurácia: {}\".format(accuracy))\n",
    "                y_pred = np.argmax(model.predict(X_test_final), axis=-1)\n",
    "                keras.backend.reset_uids()\n",
    "\n",
    "                compile_results('cnn',nn_params,custom_embedding_name,y_test, y_pred, encoder.classes_)\n",
    "            \n",
    "def lstm():\n",
    "\n",
    "    #LSTM\n",
    "    units_list = [10,20,50,70,100]\n",
    "\n",
    "    for word_embeddings in custom_word_embeddings:\n",
    "        for units in units_list:\n",
    "\n",
    "            # load the whole embedding into memory\n",
    "            embeddings_index = dict()\n",
    "            f = open(word_embeddings)\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = coefs\n",
    "            f.close()\n",
    "\n",
    "            #Número de dimensões da W.E.\n",
    "            num_dim = len(coefs)\n",
    "\n",
    "            #print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "            # create a weight matrix for words in training docs\n",
    "            embedding_matrix = np.zeros((vocab_size, num_dim))\n",
    "            for word, i in t.word_index.items():\n",
    "                embedding_vector = embeddings_index.get(word)\n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "\n",
    "            custom_embedding_name = word_embeddings.split('/')[-1].replace('.txt','')\n",
    "\n",
    "            nn_params = \"Embedding_out_dim = {}| LSTM_units={}\".format(num_dim, units)\n",
    "            print(nn_params)\n",
    "\n",
    "            model = Sequential()\n",
    "            model.add(Embedding(vocab_size, num_dim, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "            model.add(LSTM(units,return_sequences=False))\n",
    "            model.add(Dropout(0.5))\n",
    "            model.add(Dense(num_classes, activation='softmax'))\n",
    "            #model.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "            model.compile(loss='mse', optimizer='adam')\n",
    "            #model.summary()\n",
    "            #print()\n",
    "            history = model.fit(X_train_final, y_train, epochs=100, batch_size=32, validation_data=(X_test_final,y_test), verbose=0, shuffle=False)\n",
    "            #avaliando a rede\n",
    "            #loss, accuracy = model.evaluate(X_test_final, y_test)\n",
    "            #print(\"Acurácia: {}\".format(accuracy))\n",
    "            y_pred = np.argmax(model.predict(X_test_final), axis=-1)\n",
    "            keras.backend.reset_uids()\n",
    "\n",
    "            compile_results('lstm',nn_params,custom_embedding_name,y_test, y_pred, encoder.classes_)\n",
    "\n",
    "def lstm_return_sequences_average_pooling():\n",
    "    #LSTM return_sequences_average_pooling\n",
    "    units_list = [10,20,50,70,100]\n",
    "\n",
    "    for word_embeddings in custom_word_embeddings:\n",
    "        for units in units_list:\n",
    "\n",
    "            # load the whole embedding into memory\n",
    "            embeddings_index = dict()\n",
    "            f = open(word_embeddings)\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = coefs\n",
    "            f.close()\n",
    "\n",
    "            #Número de dimensões da W.E.\n",
    "            num_dim = len(coefs)\n",
    "\n",
    "            #print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "            # create a weight matrix for words in training docs\n",
    "            embedding_matrix = np.zeros((vocab_size, num_dim))\n",
    "            for word, i in t.word_index.items():\n",
    "                embedding_vector = embeddings_index.get(word)\n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "\n",
    "            custom_embedding_name = word_embeddings.split('/')[-1].replace('.txt','')\n",
    "\n",
    "            nn_params = \"Embedding_out_dim = {}| LSTM_units={}\".format(num_dim, units)\n",
    "            print(nn_params)\n",
    "\n",
    "            model = Sequential()\n",
    "            model.add(Embedding(vocab_size, num_dim, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "            model.add(LSTM(units,return_sequences=True))\n",
    "            model.add(GlobalAveragePooling1D())\n",
    "            model.add(Dropout(0.5))\n",
    "            model.add(Dense(num_classes, activation='softmax'))\n",
    "            #model.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "            model.compile(loss='mse', optimizer='adam')\n",
    "            #model.summary()\n",
    "            #print()\n",
    "            history = model.fit(X_train_final, y_train, epochs=100, batch_size=32, validation_data=(X_test_final,y_test), verbose=0, shuffle=False)\n",
    "            #avaliando a rede\n",
    "            #loss, accuracy = model.evaluate(X_test_final, y_test)\n",
    "            #print(\"Acurácia: {}\".format(accuracy))\n",
    "            y_pred = np.argmax(model.predict(X_test_final), axis=-1)\n",
    "            keras.backend.reset_uids()\n",
    "\n",
    "            compile_results('lstm_return_sequences_average_pooling',nn_params,custom_embedding_name,y_test, y_pred, encoder.classes_)\n",
    "        \n",
    "def lstm_return_sequences_max_pooling():\n",
    "    #LSTM_return_sequences_max_pooling\n",
    "    units_list = [10,20,50,70,100]\n",
    "\n",
    "    for word_embeddings in custom_word_embeddings:\n",
    "        for units in units_list:\n",
    "\n",
    "            # load the whole embedding into memory\n",
    "            embeddings_index = dict()\n",
    "            f = open(word_embeddings)\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = coefs\n",
    "            f.close()\n",
    "\n",
    "            #Número de dimensões da W.E.\n",
    "            num_dim = len(coefs)\n",
    "\n",
    "            #print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "            # create a weight matrix for words in training docs\n",
    "            embedding_matrix = np.zeros((vocab_size, num_dim))\n",
    "            for word, i in t.word_index.items():\n",
    "                embedding_vector = embeddings_index.get(word)\n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "\n",
    "            custom_embedding_name = word_embeddings.split('/')[-1].replace('.txt','')\n",
    "\n",
    "            nn_params = \"Embedding_out_dim = {}| LSTM_units={}\".format(num_dim, units)\n",
    "            print(nn_params)\n",
    "\n",
    "            model = Sequential()\n",
    "            model.add(Embedding(vocab_size, num_dim, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "            model.add(LSTM(units,return_sequences=True))\n",
    "            model.add(GlobalMaxPool1D())\n",
    "            model.add(Dropout(0.5))\n",
    "            model.add(Dense(num_classes, activation='softmax'))\n",
    "            #model.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "            model.compile(loss='mse', optimizer='adam')\n",
    "            #model.summary()\n",
    "            #print()\n",
    "            history = model.fit(X_train_final, y_train, epochs=100, batch_size=32, validation_data=(X_test_final,y_test), verbose=0, shuffle=False)\n",
    "            #avaliando a rede\n",
    "            #loss, accuracy = model.evaluate(X_test_final, y_test)\n",
    "            #print(\"Acurácia: {}\".format(accuracy))\n",
    "            y_pred = np.argmax(model.predict(X_test_final), axis=-1)\n",
    "            keras.backend.reset_uids()\n",
    "\n",
    "            compile_results('lstm_return_sequences_max_pooling',nn_params,custom_embedding_name,y_test, y_pred, encoder.classes_)\n",
    "\n",
    "def gru():\n",
    "    #GRU\n",
    "    units_list = [10,20,50,70,100]\n",
    "\n",
    "    for word_embeddings in custom_word_embeddings:\n",
    "        for units in units_list:\n",
    "\n",
    "            # load the whole embedding into memory\n",
    "            embeddings_index = dict()\n",
    "            f = open(word_embeddings)\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = coefs\n",
    "            f.close()\n",
    "\n",
    "            #Número de dimensões da W.E.\n",
    "            num_dim = len(coefs)\n",
    "\n",
    "            #print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "            # create a weight matrix for words in training docs\n",
    "            embedding_matrix = np.zeros((vocab_size, num_dim))\n",
    "            for word, i in t.word_index.items():\n",
    "                embedding_vector = embeddings_index.get(word)\n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "\n",
    "            custom_embedding_name = word_embeddings.split('/')[-1].replace('.txt','')\n",
    "\n",
    "            nn_params = \"Embedding_out_dim = {}| GRU_units={}\".format(num_dim, units)\n",
    "            print(nn_params)\n",
    "\n",
    "            model = Sequential()\n",
    "            model.add(Embedding(vocab_size, num_dim, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "            model.add(GRU(units,return_sequences=False))\n",
    "            model.add(Dropout(0.5))\n",
    "            model.add(Dense(num_classes, activation='softmax'))\n",
    "            #model.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "            model.compile(loss='mse', optimizer='adam')\n",
    "            #model.summary()\n",
    "            #print()\n",
    "            history = model.fit(X_train_final, y_train, epochs=100, batch_size=32, validation_data=(X_test_final,y_test), verbose=0, shuffle=False)\n",
    "            #avaliando a rede\n",
    "            #loss, accuracy = model.evaluate(X_test_final, y_test)\n",
    "            #print(\"Acurácia: {}\".format(accuracy))\n",
    "            y_pred = np.argmax(model.predict(X_test_final), axis=-1)\n",
    "            keras.backend.reset_uids()\n",
    "\n",
    "            compile_results('gru',nn_params,custom_embedding_name,y_test, y_pred, encoder.classes_)\n",
    "\n",
    "def gru_return_sequences_average_pooling():\n",
    "    #GRU_return_sequences_average_pooling\n",
    "    units_list = [10,20,50,70,100]\n",
    "\n",
    "    for word_embeddings in custom_word_embeddings:\n",
    "        for units in units_list:\n",
    "\n",
    "            # load the whole embedding into memory\n",
    "            embeddings_index = dict()\n",
    "            f = open(word_embeddings)\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = coefs\n",
    "            f.close()\n",
    "\n",
    "            #Número de dimensões da W.E.\n",
    "            num_dim = len(coefs)\n",
    "\n",
    "            #print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "            # create a weight matrix for words in training docs\n",
    "            embedding_matrix = np.zeros((vocab_size, num_dim))\n",
    "            for word, i in t.word_index.items():\n",
    "                embedding_vector = embeddings_index.get(word)\n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "\n",
    "            custom_embedding_name = word_embeddings.split('/')[-1].replace('.txt','')\n",
    "\n",
    "            nn_params = \"Embedding_out_dim = {}| GRU_units={}\".format(num_dim, units)\n",
    "            print(nn_params)\n",
    "\n",
    "            model = Sequential()\n",
    "            model.add(Embedding(vocab_size, num_dim, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "            model.add(GRU(units,return_sequences=True))\n",
    "            model.add(GlobalAveragePooling1D())\n",
    "            model.add(Dropout(0.5))\n",
    "            model.add(Dense(num_classes, activation='softmax'))\n",
    "            #model.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "            model.compile(loss='mse', optimizer='adam')\n",
    "            #model.summary()\n",
    "            #print()\n",
    "            history = model.fit(X_train_final, y_train, epochs=100, batch_size=32, validation_data=(X_test_final,y_test), verbose=0, shuffle=False)\n",
    "            #avaliando a rede\n",
    "            #loss, accuracy = model.evaluate(X_test_final, y_test)\n",
    "            #print(\"Acurácia: {}\".format(accuracy))\n",
    "            y_pred = np.argmax(model.predict(X_test_final), axis=-1)\n",
    "            keras.backend.reset_uids()\n",
    "\n",
    "            compile_results('gru_return_sequences_average_pooling',nn_params,custom_embedding_name,y_test, y_pred, encoder.classes_)\n",
    "\n",
    "def gru_return_sequences_max_pooling():\n",
    "    #GRU_return_sequences_max_pooling\n",
    "    units_list = [10,20,50,70,100]\n",
    "\n",
    "    for word_embeddings in custom_word_embeddings:\n",
    "        for units in units_list:\n",
    "\n",
    "            # load the whole embedding into memory\n",
    "            embeddings_index = dict()\n",
    "            f = open(word_embeddings)\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = coefs\n",
    "            f.close()\n",
    "\n",
    "            #Número de dimensões da W.E.\n",
    "            num_dim = len(coefs)\n",
    "\n",
    "            #print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "            # create a weight matrix for words in training docs\n",
    "            embedding_matrix = np.zeros((vocab_size, num_dim))\n",
    "            for word, i in t.word_index.items():\n",
    "                embedding_vector = embeddings_index.get(word)\n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "\n",
    "            custom_embedding_name = word_embeddings.split('/')[-1].replace('.txt','')\n",
    "\n",
    "            nn_params = \"Embedding_out_dim = {}| GRU_units={}\".format(num_dim, units)\n",
    "            print(nn_params)\n",
    "\n",
    "            model = Sequential()\n",
    "            model.add(Embedding(vocab_size, num_dim, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "            model.add(GRU(units,return_sequences=True))\n",
    "            model.add(GlobalMaxPool1D())\n",
    "            model.add(Dropout(0.5))\n",
    "            model.add(Dense(num_classes, activation='softmax'))\n",
    "            #model.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "            model.compile(loss='mse', optimizer='adam')\n",
    "            #model.summary()\n",
    "            #print()\n",
    "            history = model.fit(X_train_final, y_train, epochs=100, batch_size=32, validation_data=(X_test_final,y_test), verbose=0, shuffle=False)\n",
    "            #avaliando a rede\n",
    "            #loss, accuracy = model.evaluate(X_test_final, y_test)\n",
    "            #print(\"Acurácia: {}\".format(accuracy))\n",
    "            y_pred = np.argmax(model.predict(X_test_final), axis=-1)\n",
    "            keras.backend.reset_uids()\n",
    "\n",
    "            compile_results('gru_return_sequences_max_pooling',nn_params,custom_embedding_name,y_test, y_pred, encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-02T20:51:08.816831Z",
     "iopub.status.busy": "2021-07-02T20:51:08.816279Z",
     "iopub.status.idle": "2021-07-02T21:58:22.498307Z",
     "shell.execute_reply": "2021-07-02T21:58:22.498788Z"
    },
    "papermill": {
     "duration": 4033.689483,
     "end_time": "2021-07-02T21:58:22.499012",
     "exception": false,
     "start_time": "2021-07-02T20:51:08.809529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding_out_dim = 300| GRU_units=10\n",
      "Embedding_out_dim = 300| GRU_units=20\n",
      "Embedding_out_dim = 300| GRU_units=50\n",
      "Embedding_out_dim = 300| GRU_units=70\n",
      "Embedding_out_dim = 300| GRU_units=100\n",
      "Embedding_out_dim = 100| GRU_units=10\n",
      "Embedding_out_dim = 100| GRU_units=20\n",
      "Embedding_out_dim = 100| GRU_units=50\n",
      "Embedding_out_dim = 100| GRU_units=70\n",
      "Embedding_out_dim = 100| GRU_units=100\n"
     ]
    }
   ],
   "source": [
    "gru_return_sequences_max_pooling()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4056.972189,
   "end_time": "2021-07-02T21:58:25.931343",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-07-02T20:50:48.959154",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
